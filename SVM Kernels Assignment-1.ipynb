{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5573ca-9cc8-44a5-9af4-4f4cd9d46ddd",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4056029-e9a5-4d5c-96c8-0ed051f99fbe",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) is:\n",
    "\n",
    "(\n",
    ")\n",
    "=\n",
    "sign\n",
    "(\n",
    "⋅\n",
    "+\n",
    ")\n",
    "f(x)=sign(w⋅x+b)\n",
    "Where:\n",
    "(\n",
    ")\n",
    "f(x) is the decision function that predicts the class of a given input \n",
    "x.\n",
    "w is the weight vector.\n",
    "x is the input vector.\n",
    "b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015fa82-0a36-465b-9cba-d3514121c5f1",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a611a-a52a-42c8-b38b-8160b544e72b",
   "metadata": {},
   "source": [
    " The objective function of a linear SVM is to maximize the margin between the decision boundary and the support vectors while minimizing the classification error. It can be represented as:\n",
    "\n",
    "min\n",
    "\n",
    ",\n",
    "1\n",
    "2\n",
    "∣\n",
    "∣\n",
    "∣\n",
    "∣\n",
    "2\n",
    "w,b\n",
    "min\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∣∣w∣∣ \n",
    "2\n",
    " \n",
    "subject to the constraints:\n",
    "\n",
    "(\n",
    "⋅\n",
    "+\n",
    ")\n",
    "≥\n",
    "1\n",
    ",\n",
    "for \n",
    "=\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1,for i=1,2,...,n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d0d11-2a16-433b-a16a-0c2c746b2374",
   "metadata": {},
   "source": [
    "where \n",
    "w is the weight vector, \n",
    "b is the bias term, \n",
    "x \n",
    "i\n",
    "​\n",
    "  is the \n",
    "i-th training example, \n",
    "y \n",
    "i\n",
    "​\n",
    "  is the corresponding class label, and \n",
    "n is the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7d117-2026-452d-85e8-6f06d3dc6457",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf105f-3e28-42f8-b3cd-90fdb5c66ef5",
   "metadata": {},
   "source": [
    " The kernel trick in SVM is a method used to handle non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space where it becomes linearly separable. This is achieved by defining a kernel function \n",
    "(\n",
    ",\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ) that computes the inner product of the mapped data without explicitly computing the mapping itself. Common kernel functions include the linear kernel, polynomial kernel, Gaussian (RBF) kernel, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a3351-2d19-4240-89b2-fcf4a025a398",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f6d22-f0c9-4cd6-afae-4f913146bd6e",
   "metadata": {},
   "source": [
    " Support vectors in SVM are the data points that lie closest to the decision boundary (margin). They play a crucial role in defining the decision boundary as they are the only data points that contribute to the determination of the margin and the decision function. Other data points do not influence the decision boundary. For example, in a binary classification problem, if there are two classes, support vectors would be the data points from both classes that are closest to the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94689a0-84e5-43a1-91b0-17833f163b1d",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725918c-6a90-4242-a92d-671784ab4973",
   "metadata": {},
   "source": [
    "# Illustration of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM:\n",
    "\n",
    "Hyperplane: In SVM, a hyperplane is a decision boundary that separates classes in a feature space. For a binary classification problem, it is a (d-1)-dimensional subspace where 'd' is the number of features. In a 2D space, it's a line; in 3D, it's a plane.\n",
    "\n",
    "Marginal plane: The marginal plane is the hyperplane that maximizes the margin between the support vectors of different classes. It is equidistant from the nearest support vectors of each class.\n",
    "\n",
    "Soft margin: Soft margin SVM allows for some misclassifications to achieve a more generalizable decision boundary. It introduces a penalty parameter \n",
    "\n",
    "C that controls the trade-off between maximizing the margin and minimizing the classification error. This allows for a certain degree of flexibility in the placement of the decision boundary.\n",
    "\n",
    "Hard margin: Hard margin SVM aims to find the maximum-margin hyperplane without allowing any misclassifications. It is suitable for linearly separable data. However, it may not be suitable for noisy or overlapping data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10656b6-65f4-437f-95b7-6dd24299c3de",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dc14f-8644-4a08-a223-2ae36b196d3d",
   "metadata": {},
   "source": [
    "# SVM Implementation through Iris dataset:\n",
    "\n",
    "Let's implement a linear SVM classifier using the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b1310-9377-482d-b3d4-d59a4e55d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
